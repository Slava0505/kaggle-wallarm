{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e81c561825e8d0166e82a3e1a41e95d56b4abbcc"
   },
   "source": [
    "### General information\n",
    "\n",
    "In this kernel I'll analyse data from Malicious Intent Detection Challenge.\n",
    "\n",
    "We need to identify injections among neutral input vectors using machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "%matplotlib inline\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy as sp\n",
    "\n",
    "import eli5\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "pd.set_option('max_colwidth',400)\n",
    "import msgpack\n",
    "from sklearn.decomposition import  PCA,TruncatedSVD\n",
    "import re\n",
    "from sklearn.cluster import MiniBatchKMeans, DBSCAN, KMeans\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "245942e9b5048b13690ed1dbadbcb4ab0702cb05"
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "info = pd.read_csv('data/train_info.csv')\n",
    "with open('data/train_msgpack.msgpack', 'rb') as data_file:\n",
    "    train = msgpack.unpack(data_file)\n",
    "with open('data/test_msgpack.msgpack', 'rb') as data_file:\n",
    "    test = msgpack.unpack(data_file)\n",
    "\n",
    "train = pd.DataFrame(train)\n",
    "test = pd.DataFrame(test)\n",
    "train.columns = ['id', 'text']\n",
    "test.columns = ['id', 'text']\n",
    "\n",
    "train['text'] = train['text'].astype(str)\n",
    "test['text'] = test['text'].astype(str)\n",
    "\n",
    "tst = train['text'].copy()\n",
    "\n",
    "train['len'] = train.text.apply(lambda x: len(x))\n",
    "test['len'] = test.text.apply(lambda x: len(x))\n",
    "\n",
    "train['new_text'] = train.text.str.replace(r'[^a-zA-Z ]', ' ').str.split()\n",
    "test['new_text'] = test.text.str.replace(r'[^a-zA-Z ]', ' ').str.split()\n",
    "\n",
    "train['len_new_text'] = train['new_text'].apply(lambda x: len(x))\n",
    "test['len_new_text'] = test['new_text'].apply(lambda x: len(x))\n",
    "\n",
    "train['new_text']  = train['new_text'].apply(lambda x: ' '.join([i for i in x if len(i)>2]))\n",
    "test['new_text']  = test['new_text'].apply(lambda x: ' '.join([i for i in x if len(i)>2]))\n",
    "# test features\n",
    "train['num_low'] = train['text'].str.replace(r'[^a-z]', '').str.len()\n",
    "test['num_low'] = test['text'].str.replace(r'[^a-z]', '').str.len()\n",
    "\n",
    "train['num_up'] = train['text'].str.replace(r'[^A-Z]', '').str.len()\n",
    "test['num_up'] = test['text'].str.replace(r'[^A-Z]', '').str.len()\n",
    "\n",
    "train['num_letters'] = train['num_up']+train['num_low']\n",
    "test['num_letters'] = test['num_up']+test['num_low']\n",
    "\n",
    "train['numbers'] = train['text'].str.replace(r'[^\\d ]', ' ')\n",
    "test['numbers'] = test['text'].str.replace(r'[^\\d ]', '')\n",
    "\n",
    "train['numbers_list'] = train['numbers'].str.split()\n",
    "test['numbers_list'] = test['numbers'].str.split()\n",
    "\n",
    "train['numbers'] = train['numbers'].apply(lambda x: len(x))\n",
    "test['numbers'] = test['numbers'].apply(lambda x: len(x))\n",
    "\n",
    "train['numbers_1'] = train['numbers_list'].apply(lambda x: len([i for i in x if len(i)==1]))\n",
    "test['numbers_1'] = test['numbers_list'].apply(lambda x: len([i for i in x if len(i)==1]))\n",
    "\n",
    "train['numbers_2'] = train['numbers_list'].apply(lambda x: len([i for i in x if len(i)==2]))\n",
    "test['numbers_2'] = test['numbers_list'].apply(lambda x: len([i for i in x if len(i)==2]))\n",
    "\n",
    "train['numbers_3'] = train['numbers_list'].apply(lambda x: len([i for i in x if len(i)==3]))\n",
    "test['numbers_3'] = test['numbers_list'].apply(lambda x: len([i for i in x if len(i)==3]))\n",
    "\n",
    "train['numbers_more'] = train['numbers_list'].apply(lambda x: len([i for i in x if len(i)>3]))\n",
    "test['numbers_more'] = test['numbers_list'].apply(lambda x: len([i for i in x if len(i)>3]))\n",
    "\n",
    "all_chars = set()\n",
    "for text in train.text:\n",
    "    all_chars = all_chars | set(text)\n",
    "    \n",
    "for char in all_chars:\n",
    "    train['num_'+char] = train.text.apply(lambda x: x.count(char))\n",
    "    test['num_'+char] = test.text.apply(lambda x: x.count(char))\n",
    "    \n",
    "train['text'] = train['text'].str.replace(r'\\d{4,}', 'ࠉ').str.replace(r'\\d{3,}', 'ࠈ').str.replace(r'\\d{2,}', 'ࠇ').str.replace(r'\\d{1,}', 'ࠔ')\n",
    "test['text'] = test['text'].str.replace(r'\\d{4,}', 'ࠉ').str.replace(r'\\d{3,}', 'ࠈ').str.replace(r'\\d{2,}', 'ࠇ').str.replace(r'\\d{1,}', 'ࠔ')\n",
    "\n",
    "info = pd.merge(train, info, on='id')\n",
    "y = np.array([1 if i == True else 0 for i in info.injection.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "8cbb0d0148e62e9428028982041093a3b4904d9c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.2 s, sys: 684 ms, total: 29.9 s\n",
      "Wall time: 29.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(use_idf=True,\n",
    "    ngram_range=(1, 2), analyzer='char', norm=None,  min_df = 0.001,max_df = 0.7)\n",
    "\n",
    "full_text = list(train['text'].values) + list(test['text'].values)\n",
    "vectorizer.fit(full_text)\n",
    "train_vectorized1 = vectorizer.transform(train['text'])\n",
    "test_vectorized1 = vectorizer.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.32 s, sys: 44.1 ms, total: 4.37 s\n",
      "Wall time: 4.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(use_idf=True,\n",
    "    ngram_range=(1, 1), analyzer='word', norm=None,  min_df = 0.004,max_df = 0.7)\n",
    "\n",
    "full_text = list(train['new_text'].values) + list(test['new_text'].values)\n",
    "vectorizer.fit(full_text)\n",
    "train_vectorized_text = vectorizer.transform(train['new_text'])\n",
    "test_vectorized_text = vectorizer.transform(test['new_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add = ['len','len_new_text']+['num_'+i for i in list(all_chars) if i not in [str(j) for j in range(10)]]\\\n",
    "+ ['num_low','num_up','numbers_1','numbers_2','numbers_3','numbers_more','numbers']\n",
    "\n",
    "new_train = train[columns_to_add]\n",
    "new_test = test[columns_to_add]\n",
    "\n",
    "new_train = sp.sparse.hstack((new_train,train_vectorized1), format = 'csr')\n",
    "new_test = sp.sparse.hstack((new_test,test_vectorized1), format = 'csr')\n",
    "\n",
    "new_train = sp.sparse.hstack((new_train,train_vectorized_text), format = 'csr')\n",
    "new_test = sp.sparse.hstack((new_test,test_vectorized_text), format = 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean auc 99.9865%, std 0.0036.\n",
      "CPU times: user 21min, sys: 3.11 s, total: 21min 4s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lgb = LGBMClassifier(boosting = 'dart',#goss\n",
    "                     n_estimators = 300,\n",
    "                     learning_rate=0.25,\n",
    "#                      n_estimators = 700,\n",
    "#                      learning_rate=0.03,\n",
    "                     random_state = 2,\n",
    "                     feature_fraction = 0.85,\n",
    "                     num_leaves = 100,\n",
    "#                      max_bin=200,\n",
    "                     min_data_in_leaf = 90,\n",
    "                     \n",
    "                    )\n",
    "scores = cross_val_score(lgb, new_train, y, scoring='roc_auc', cv=5)\n",
    "print('Cross-validation mean auc {0:.4f}%, std {1:.4f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a1f5942b76dc1194b2c6d479ab0bef4c1f00b50"
   },
   "outputs": [],
   "source": [
    "lgb.fit(new_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b2bfae5dbaf951bac5feb9f4a5a0f80e883c3d9f"
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('data/sample_submission.csv')\n",
    "pred = lgb.predict_proba(new_test)\n",
    "sub['injection'] = pred\n",
    "sub.head()\n",
    "sub.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
